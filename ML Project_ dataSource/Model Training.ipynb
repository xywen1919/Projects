{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split #sklearn.cross_validation for lower python versions\n",
    "from collections import defaultdict, Counter\n",
    "from pprint import pprint\n",
    "from sklearn import metrics\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.externals import joblib\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And tune pandas display options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 3000)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Local use set Current Working directory path\n",
    "# os.chdir('<path>')\n",
    "\n",
    "# Current working directory must have all the input files required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd().strip() + '/training_input (IQVIA derivation dataset).csv' #Input for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing Model Input File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading dataset from defined path:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = pd.read_csv(path)\n",
    "print ('Base data has %i rows and %i columns' % (model_input.shape[0], model_input.shape[1]))\n",
    "print ('Displaying first 5 rows')\n",
    "model_input.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking if there are any empty cells in model_input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_input.isnull().values.any() == 'True':\n",
    "    raise ValueError('Model Input File has null values')\n",
    "else:\n",
    "    print ('There are no empty values in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Level of data is patient_id. Each row uniquely identifies a patient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = model_input.shape[0]\n",
    "patient_count = len(model_input['patient_id'].unique())\n",
    "if row_count == patient_count:\n",
    "    print ('Row Count: ', model_input.shape[0])\n",
    "    print ('Patient Count: ', len(model_input['patient_id'].unique()))\n",
    "else:\n",
    "    raise ValueError('Model Input File is not at required level of data (patient_id)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATTR WT patients are matched 1:1 with HF using Propensity Score Matching giving us 817 patients for each cohort ATTR WT and HF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input.groupby(['cohort_type','cohort_flag']).patient_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cohort_flag is 1 for ATTR WT patients. We would drop cohort_type and use cohort_flag as target for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping columns not required for training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = model_input.drop(['patient_id','cohort_type'], axis = 1)\n",
    "model_input.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are now going to handle the target variable (cohort_flag) and store it in a new variable:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_map = {u'1': 1, u'0': 0}\n",
    "model_input['__target__'] = model_input['cohort_flag'].map(str).map(target_map)\n",
    "model_input = model_input.drop(['cohort_flag'], axis = 1)\n",
    "\n",
    "model_input.groupby(['__target__']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_target_leaks = ['cardiomyopathy_in_diseases_classified_elsewhere','other_forms_of_heart_disease']\n",
    "\n",
    "model_input_flt_leaks = model_input.drop(potential_target_leaks, axis = 1)\n",
    "model_input_flt_leaks.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting model_input into test and train**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset needs to be split into 2 new sets, one that will be used for training the model (train set)\n",
    "and another that will be used to test its generalization capability (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_input_flt_leaks.drop('__target__', axis=1)\n",
    "y = np.array(model_input_flt_leaks['__target__'])\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print ('Train data has %i rows and %i columns' % (train_X.shape[0], train_X.shape[1]))\n",
    "print ('Test data has %i rows and %i columns' % (test_X.shape[0], test_X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(train_X).to_csv('train_X.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Displaying all features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(train_X)\n",
    "print (len(features), ' features')\n",
    "display(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 500, num = 4)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 20, num = 4)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2,3,5,7]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [2,3,4,5]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "total_combinations = len(n_estimators)*len(max_features)*len(max_depth)*len(min_samples_split)*len(min_samples_leaf)*len(bootstrap)\n",
    "print('Trying out total combinations: ', total_combinations)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(random_state=10000)\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across different combinations\n",
    "%time rf_grid = GridSearchCV(estimator = clf, param_grid = param_grid, cv = 3, verbose=2, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "%time rf_grid.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestparams_clf=rf_grid.best_params_\n",
    "bestparams_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=bestparams_clf['n_estimators'],random_state= 10000, min_samples_leaf= bestparams_clf['min_samples_leaf'])\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = rf_grid.best_estimator_\n",
    "_clf_base_predictions = best_grid.predict(test_X)\n",
    "_clf_base_probas = best_grid.predict_proba(test_X)\n",
    "\n",
    "test_y_series = pd.Series(data=test_y, index=test_X.index, name='cohort_flag')\n",
    "test_y_labels = test_y_series.to_frame()\n",
    "\n",
    "clf_base_predictions = pd.Series(data=_clf_base_predictions, index=test_X.index, name='predicted_value')\n",
    "cols = [\n",
    "    u'probability_of_value_%s' % label\n",
    "    for (_, label) in sorted([(int(target_map[label]), label) for label in target_map])\n",
    "]\n",
    "\n",
    "clf_base_probabilities = pd.DataFrame(data=_clf_base_probas, index=test_X.index, columns=cols)\n",
    "\n",
    "clf_base_results_test = test_y_labels.join(clf_base_predictions, how='left')\n",
    "clf_base_results_test = clf_base_results_test.join(clf_base_probabilities, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model performance metrics\n",
    "\n",
    "print ('Accuracy:'),print(metrics.accuracy_score(test_y, _clf_base_predictions))\n",
    "print ('\\nF1 Score:'),print(metrics.f1_score(test_y, _clf_base_predictions))\n",
    "print ('\\nRecall:'),print(metrics.recall_score(test_y, _clf_base_predictions))\n",
    "print ('\\nPrecision:'),print(metrics.precision_score(test_y, _clf_base_predictions))\n",
    "print ('\\nClassification report:'),print(metrics.classification_report(test_y, _clf_base_predictions))\n",
    "print ('\\nConfusion matrix:'),print(metrics.confusion_matrix(test_y, _clf_base_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_base_results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Area under curve:', metrics.roc_auc_score(clf_base_results_test['cohort_flag'],clf_base_results_test['probability_of_value_1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_data = []\n",
    "features = train_X.columns\n",
    "for feature_name, feature_importance in zip(features, clf.feature_importances_):\n",
    "    feature_importances_data.append({\n",
    "        'feature': feature_name,\n",
    "        'importance': feature_importance\n",
    "    })\n",
    "\n",
    "# Plot the results\n",
    "pd.DataFrame(feature_importances_data)\\\n",
    "    .set_index('feature')\\\n",
    "    .sort_values(by='importance')[-10::]\\\n",
    "    .plot(title='Top 10 most important variables',\n",
    "          kind='barh',\n",
    "          figsize=(10, 6),\n",
    "          color='#348ABD',\n",
    "          alpha=0.6,\n",
    "          lw='1',\n",
    "          edgecolor='#348ABD',\n",
    "          grid=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the feature importance\n",
    "\n",
    "pd.DataFrame(feature_importances_data).to_csv('feature_importance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle = dataiku.Folder(\"0h3MrWHS\")\n",
    "path = os.getcwd()\n",
    "\n",
    "filename = path+'/rf_best_excl_1_CM_3_6.sav'\n",
    "joblib.dump(clf, filename)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "creator": "velins02",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
