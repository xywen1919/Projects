---
title: "Assignment 2"
author: "Xiaoyan Wen"
date: "2/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```
# Next Generation Sequence Analysis Homework Week 2
Pre-processing of raw fastq data typically involves some combination of filtering of low quality reads, adapter trimming, trimming of low quality bases, and in some applications removal of contaminating reads from non-target sources. The best approach for read processing is dependent on the application and its impact on results is often uncertain. It is therefore important to carefully consider available best practices for the application before proceeding with data pre-processing strategy. Here you will have the opportunity to process short read re-sequencing (Illumina) data and summarizing the processed files.

## About the data
This week we are beginning a multi-week assignment where we will use short read re-sequencing data from the 1000 Genomes Project to call snps and genotypes and perform population genomic analysis.
The short read sequence data you will process was generated by the 1000 Genomes Project. https://www.internationalgenome.org/
Details about the fastqs and how they were generated are here: https://www.internationalgenome.org/category/fastq/

## General considerations for fastq pre-processing
Raw reads from NGS platforms typically require some form of quality control. PacBio long reads, for example, often require some form of error correction. Here we will list some general considerations for short reads (Illumina). There are a few rules of thumb when considering short read data pre-processing:

- With the exception of some applications that use universal molecular identifiers (UMIs), adapter sequences which may include barcodes (in a multiplexed experiment) **should not appear** in raw fastq files that are often the starting point for bioinformatic analysis. Therefore, an adapter sequence in the reads is an artifact of the library preparation and sequencing process and should be removed with an appropriate tool

- Whether to trim low quality bases is application dependant

- It is important to consider the effect of mapping processed reads shorter than 50 bp 

- Removal of reads failing platform-specific QC checks is generally a good idea

Whether or not to trim to remove low quality bases (typically from the ends of the reads) depends on the application. For example, in a typical re-sequencing study (e.g., 1,000 Genomes Project), low base qualities are unlikely to be a serious problem with modern snp-calling and genotyping software. 
- First, most workflows now use alignment tools that soft-clip low quality bases that dont match a reference (effectively removing them from consideration) as we shall see in Week 3. 
- Second, modern variant discovery tools such as the **GATK** use base-qualities in the maximum likelihood calculations used to call a genotype. In fact, the Genome Analysis Tool Kit (GATK) developed at the Broad Institute (which we will use in for snp-calling), does not recommend trimming for base quality.

RNA-seq results have been found to be adversely impacted by overly aggressive base quality trimming (see pre-recorded videos) and how stringent to trim the reads, if at all, should be considered.

A primary consideration when trimming either for adapters or base quality is that trimming shortens the reads and increases the probability of mismapping to a reference genome. This can be problematic. In most cases where reads are to be aligned to a reference, a minimum length filter should be applied that requires reads to be a certain length (e.g., > 50 bp depending on aligner and application) in order to be retained for downstream steps. Mis-mapping is an important source of error in NGS applications that map reads to a reference genome and can impact conclusions, so must always be considered and its impact assessed when necessary.

It is important to emphasize that base quality trimming and other QC considerations are highly dependent on the application and it is important to consider current knowledge on best practices for each application.

There are many viable approaches to trimming reads for re-sequencing applications. The most common is to process reads as we will do in this exercise with a third party tool or combination of tools. You may encounter other strategies however. For example, the **Broad Institute**, has an in house workflow for processing re-sequencing data that has a unique strategy for pre-processing data for variant discovery (https://software.broadinstitute.org/gatk/documentation/article?id=6483). Raw data are stored as **uBAM** (rather than fastq) and a custom tool, **MarkIlluminaAdapters**, is used to flag positions in reads that match adapters used in the sequencing experiment (which must be known ahead of time). Downstream **GATK tools** can then use the “marked” uBAMs during downstream processing steps. The Broad Institute approach is somewhat unusual in that it marks the positions of adapters instead of removing them.

Here, we will use a more conventional approach that will use a third party tool to trim adapters, remove any low quality reads (failing Illuminas quality control), and apply a minimum length threshold to fastqs from the 1000 Genomes Project. We will use a read trimming software called **fastp** (Chen et al. 2018) to perform these tasks.

We will quality control different types of data later in the course and will return to some of these issues in future assignments.

## Task 1: Pre-processing fastq data for variant discovery
**fastp** is a tool that integrates many of the benefits of older tools such as Trimmomatic (frequently used in base quality trimming) and cutadapt (frequently used for adapter trimming), while adding additional features. For example, fastp automatically detects Illumina adapters without the need to pass a library of adapter sequences and can eliminate artifacts (i.e., polyG sequences; see pre-recorded videos) that are specific to the latest Illumina sequencing platforms such as NextSeq and NovaSeq.

Begin by reviewing the fastp Github page in your browser. https://github.com/OpenGene/fastp
You will run fastp on the fastq files for human sample HG00149 located on Greene at the following paths.
```{bash}
/scratch/work/courses/BI7653/hw2.2022/ERR156634_1.filt.fastq.gz
/scratch/work/courses/BI7653/hw2.2022/ERR156634_2.filt.fastq.gz
```

It is best practice NOT to copy the fastq files (or any other large file) from its existing location in the course directory to your /scratch. Instead, you can simply paste the full paths into the input file arguments to fastp. This prevents unnecessary redundancy of large files on the HPC and preserves disk space.

You will now create a job submission script to execute fastp on the above paired end files.
Begin by logging into Greene and checking out a compute node.
```{bash}
srun --time=4:00:00 --mem=4GB --pty /bin/bash
```

Now create a new directory **“ngs.week2”** in your /scratch directory and a subdirectory for each tasks 1-3 below.

Next cd to your Task 1 directory, e.g., 
```{bash}
$SCRATCH/ngs.week2/task1
```

copy the slurm template at the following directory and rename it.
```{bash}
/scratch/work/courses/BI7653/hw1.2022/slurm_template.sh
cp /scratch/work/courses/BI7653/hw1.2022/slurm_template.sh .  
# the "." refers to the present working directory

mv slurm_template.sh <name of your choosing>
```

Now execute an appropriate fastp commandline via a job submission (“slurm”) script as follows:

1. Modify the template script to request 4 Gb of memory and 4 hrs runtime in the #SBATCH directives
2. Modify the template script to load the fastp module
3. Add a fastp command to execute fastp on the pair of fastqs listed above. Use the “simple usage” examples on the fastp Github page to run fastp on paired-ends
4. Define arguments -i, -I, -o, -O with appropriate values.
5. Please also add the following arguments to your fastp command line:

```{bash}
--length_required 76  # retain only reads  76 bp or longer after trimming as recommended for BWA-MEM aligner

--n_base_limit 50     # tolerate up to 50 Ns in reads

--detect_adapter_for_pe # turn on paired-end adapter trimming
```

Execute your script using sbatch

Follow job status using:
```{bash}
squeue -u <net id>
```

**Questions**
Q1.1 For your answer report the following [ 1 point ].

Q1.1a.Paste the contents of your job script into your homework file. Please use an RMarkdown code block or equivalent (see Week 2 webinar when we will discuss RMarkdown) if possible.

<your script here>
Q1.1b Report your job id.

Q1.1c Check the exit status of your job usig the seff command. What is the exit code and what does it mean?

Q1.2 A common source of confusion for students new to the command prompt is the distinction between relative and absolute file paths. [ 1 point ] Q1.2a What is the difference?

Q1.2b Did you use an absolute or relative path to read the fastq files in the course directory? Did you use an absolute or relative path to write the processed fastq files?

During read processing, it is common that a tool such as fastp will filter a read from one fastq file (e.g., because it is shorter than the minimum length), but its mated read in the second file will NOT survive the filter. Preserving the number and order of short read records in paired-end fastq files is critical. Therefore, fastp will discard BOTH reads if one read fails the read length threshold (or any other filters that may have been specified).

Alternatively fastp has an option to write “orphaned” reads that pass filter (but whose mates failed filters) to additional output files should you wish to retain single-end reads for downstream analysis.

Here we have adopted the default behavior and will only work with read pairs where both reads passed QC.

Q1.3 Confirm your job has completed and that you are working interactively at a Greene compute node. You may then answer the following questions using a combination of gunzip and the STDOUT of fastp. The fastp STDOUT contains logging information. If you did not specifically re-direct STDOUT to a file when running fastp then the STDOUT will be located in the file with the STDOUT for the job. Locate that file and then answer the following [ 1 point ].

Q1.3a What is the name of the STDOUT file for your job?

Q1.3b Provide an example command line you would use to redirect the STDOUT of fastp to a file name of your choosing instead of the default STDOUT file for the job? If you are working in RMarkdown or equivalent please add a code block with your answer.

Q1.3c What percentage of the bases were Phred quality of Q30 or above in each of the original and processed fastqs?

Q1.3d Report the “Filtering result” section of the output and the duplication rate. Notice the different filters that are applied by fastp.

Q1.4. fastp produces an .html report fastp.html by default. Please download it and identify something interesting or unexpected to you and upload along with your homework document [ 1 point ].

## Task 2: Process fastqs and generate fastqc reports

Processing samples in parallel requires management of large numbers of inputs and outputs (filenames, file locations, etc.) and can get complicated for large-scale projects. Organizing project data is an essential step to working with high throughput data that is typically up to a bioinformatician or bioinformatics team. If you would like to review ideas on this topic you may refer to Noble 2009 https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000424.

In the last couple of years, it has become common to use workflow management systems such as SnakeMake https://snakemake.readthedocs.io/en/stable/# or nextflow https://www.nextflow.io/ to manage workflows including the input and about file names and locations. In fact, the rise of workflow management software and their importance in conducting reproducible research was recognized in September 2019 in a technology note in Nature (see readings, Perkel 2019; doi: 10.1038/d41586-019-02619-z).

It is important that you are aware of these strategies for managing high throughput genomic workflows. However, in this course, we will use the standard BASH shell scripting approach used by many bioinformaticians for working with small to moderate sized datasets. We will emphasize the use of job arrays to deploy these tasks on the HPC.

Job arrays use sbatch to execute a single script on multiple samples and are the best-practice for performing such “parallel” tasks on the NYU Greene cluster. This is preferred to writing for loops etc. to submit many jobs (for a variety of reasons) and is recommended by the NYU HPC team, in part, because it reduces the workload on the job scheduler and also creates an ordered set of job ids that you can use to stay organized.

Your instructor has written a job array script for you that will execute fastp and fastqc on 21 samples. The fastqs are from 2 X 100 PE runs from the 1000 Genomes Project referenced above.

Now create a directory for task2 and cd to it and copy the array job script to your present working directory.
```{bash}
$SCRATCH/ngs.week2/task2
cd <task 2 directory>
cp /scratch/work/courses/BI7653/hw2.2022/wk2_task2.sh .
```

Please update the script to include your email in the #SBATCH –mail-user argument. You can use nano, emacs, vim text editors available on Greene (see Week 1 assignment for assistance if necessary).

Now review the script contents to understand the approach taken to process samples in parallel.
```{bash}
less wk2_task2.sh  # "q" to exit when done
```

The script uses a table located at 
```{bash}
/scratch/work/courses/BI7653/hw2.2022/week2_fastqs.txt
```

to identify samples and their fastq files. This table is tab-delimited with 3 columns containing sample name, the name of the read 1 (forward) fastq file, and the name of the read 2 (reverse) fastq file. 
The fastq files are also located in the hw2.2022 directory.

Next notice the **#SBATCH –array=1-21** argument. This directive instructs slurm to launch a job array with 21 indices (parallel instances each with their own environment and variable definitions). The key environmental variable in each instance is SLURM_ARRAY_TASK_ID which will be defined in each subjob (or “job index”) as a single value between 1 and 21.

The script uses the value in SLURM_ARRAY_TASK_ID to parse the week2_fastqs.txt file by extracting the corresponding row from the table and assigning the values from columns 1, 2 and 3 to shell variables “sample”, “fq1”, and “fq2”. For example, if SLURM_ARRAY_TASK_ID variable contains a 1, the script will extract the first row from week2_fastqs.txt. If the SLURM_ARRAY_TASK_ID variable contains a 2, it will extract the second row, etc.

The remainder of the script creates a sample directory, changes directories to that directory, executes fastp and fastqc and then writes the outputs in that directory.

Before proceeding, please confirm that you are in your Task 2 directory and verify that directory is located on the /scratch file system (Do not write to your $HOME!) This is important because the trimmed fastq file outputs are large and you will quickly exceed your quota in your /home directory.

When you are ready, launch the script wk2_task2.sh as:
```{bash}
sbatch wk2_task2.sh
```

Record the job id and monitor the status of your job:`
```{bash}
squeue -u <net id>
```

You should see 21 entries with the jobid and index along with their current run status. This job should finish within 1.5 hrs or sooner for all samples (unless queue times are long).

Please confirm that the job has completed before continuing.

**Questions**
Q2.1a. Each array index will have its own STDERR and STDOUT which by default are written to a single file with naming convention slurm-.out. Please review the contents of the output for index “1” and answer the following [ 1 point ]:

Q2.1a Were any adapter sequences detected?

Q2.1b How many reads were in the read 1 set before filtering? Read 2?

Q2.1c How many reads survived filtering in Read 1 set? Read 2?

Q2.1d What percentage of reads survived filtering in Read 1 set?

Q2.1e Copy the contents of your output for index 1 of your job array to your homework file

Q2.2 Confirming quickly that each command line executed successfully can be challenging. Your instructor used echo to print the word _ESTATUS_ and the BASH special variable $? (which reports the exit status of the most recently executed command) after the fastp and fastqc commands to confirm a zero exit status.

You can quickly check that all processes completed with a zero exit status by navigating to your Task 2 directory (where the slurm-_.out files are located) and enter:
```{bash}
grep _ESTATUS_ slurm*out
```

Did all commands have an exit status of zero? Please copy the result of the grep command into your homework document [ 1 point ]


## Task 3: Use MultiQC to generate a multi-sample QC report

Quality control analysis of processed fastqs is an important step in every NGS project. Here, we take a common approach which is to combine QC analysis of all fastqs following read-trimming for the purpose of identifying outlier libraries.

Here we will use **MultiQC** to parse the outputs from FastQC outputs from Task 2 and generate a report for visual inspection.

The main page for MultiQC including several vidoes: https://multiqc.info/

The documentation is here: https://multiqc.info/docs/

Please read the section on Running MultiQC paying special attention to “Choosing Where to Scan”.

MultiQC needs to find the outputs of FastQC (with file extension fastqc.zip) from Task 2 in order to parse them. Lets get the absolute paths (i.e., from the root “/” directory) to all fastqc.zip files and write them to a file.
```{bash}
cd <your Task 2 directory>
$PWD -name \*fastqc.zip > fastqc_files.txt # adding the $PWD to the find command makes sure the file paths we extract are from root
less fastqc_files.txt # enter q to exit less
```

The fastqc_files.txt file should have paths to 42 files representing *fastqc.zip reports for forward and reverse reads for the 21 samples. We will use this to run MultiQC to generate a consolidated report for all fastqc outputs.

Create a directory for Task 3 and cd to it.

Load the most recent MultiQC software module on Greene. Then, write a command line that will execute MultiQC. You can see an example command that uses a text file, like fastqc_files.txt created above, as input to MultiQC at https://multiqc.info/docs/ under the section “Choosing Where to Scan” and use the option to “supply a file containing a list of file paths”:

Confirm you are at a compute node and execute the command from the command line. Alternatively, you could create a slurm job script and use sbatch to execute.

**Questions**
Q3.1 Report your multiqc command [ 1 point ].

Q3.2. Download the MultiQC output (multiqc_report.html by default) to your personal computer, open the report in your browser, and answer the following questions.

You can review the following short tutorial on working with MultiQC reports https://www.youtube.com/watch?v=qPbIlO_KWN0. You can hover your arrow over features in the interactive .html report to determine information like the fastq file that is represented by the feature.

Q3.2 Which fastq file has the greatest decline in base quality with increasing sequencing cycle (“the dephasing problem”)? [ 1 point ]

Q3.2c Two samples (four fastqs) appear to have unusually high GC content and unusually high duplication levels? Which samples are they? [ 1 point ]

Q3.2d Was there any residual adapter contamination in any fastq file after processing with reads with fastp [ 1 point ]?